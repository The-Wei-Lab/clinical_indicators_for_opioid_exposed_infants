{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T19:30:26.568566Z",
     "start_time": "2022-12-26T19:30:26.565306Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f8dda6-84a9-4772-9dcd-ea090463194a",
     "showTitle": false,
     "title": ""
    },
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "925e6409-2eba-468f-8d94-0a57b4ce46fa",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "# Project: Performance of phenotype algorithms for the identification of opioid-exposed infants, Andrew D. Wiese et al. Hospital Pediatrics 2024\n",
    "# Title: Project Set Up\n",
    "# Summary: \n",
    "## This is called by other notebooks to import necessary libraries, define table names, and define common functions\n",
    "# List of defined functions:\n",
    "- Change Column name case (lower or upper)\n",
    "- Dataframe inspection (unique patient and total records; local and global)\n",
    "- EGA calcuation to get standaraized EGA days\n",
    "- Get mom_baby records with code list\n",
    "- Union dataframes\n",
    "- Output df to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7e287f-e736-4403-9b23-57a15ad664ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T16:53:45.489473Z",
     "start_time": "2022-12-27T16:53:45.315305Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519e5ba9-ac5d-4df3-b625-857d9902273f",
     "showTitle": false,
     "title": ""
    },
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "7b7d432d-da12-4ca1-b42a-71583bf76c71",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "import warnings #ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd # for saving results from SQL queries to pandas DFs\n",
    "import numpy as np # for math operations\n",
    "import re # for regular expression\n",
    "import os\n",
    "import shutil\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"auto\")\n",
    "\n",
    "inspect_df_flag = 'count'\n",
    "output_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020be46e-27af-42d5-824e-b12804ff6edf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Table definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87acf358-0790-439f-a733-665df440b51c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### OMOP related tables\n",
    "proc_table=\"rd_omop_prod.procedure_occurrence\"\n",
    "cond_table=\"rd_omop_prod.condition_occurrence\"\n",
    "obs_table=\"rd_omop_prod.observation\"\n",
    "person_table=\"rd_omop_prod.person\"\n",
    "drug_exp_table=\"rd_omop_prod.drug_exposure\"\n",
    "drug_exp_table_extra=\"rd_omop_prod.x_drug_exposure\" # This is a VUMC specific table w/ additional drug exposure information\n",
    "meas_table=\"rd_omop_prod.measurement\"\n",
    "note_table=\"rd_omop_prod.note\"\n",
    "visit_table=\"rd_omop_prod.visit_occurrence\"\n",
    "concept_table=\"rd_omop_prod.concept\"\n",
    "fact_real_table=\"rd_omop_prod.fact_relationship\"\n",
    "\n",
    "### Own defined table for drug exposure\n",
    "drug_exp_table_extra=\"rd_omop_prod.x_drug_exposure\"\n",
    "\n",
    "### code lists\n",
    "lbc_code_list = \"phenotyping.mprint_live_birth_code_sheet_1\"\n",
    "critical4cpt_code_list = ['99468', '99469', '99291', '99292']\n",
    "fetal_anomalies_code_list = \"phenotyping.mprint_fetal_anomalies_code\"\n",
    "respiratory_code_list = ['96.04','96.70', '96.71', '96.72','93.90','0BH17EZ', '0BH18EZ','5A1935Z', '5A1945Z', \n",
    "   '5A1955Z','5A09357', '5A09457', '5A09557']\n",
    "infant_tox_lab_list = \"phenotyping.mprint_infant_tox_lab_name_sheet_4\"\n",
    "mom_drug_search_term_list = \"phenotyping.mprint_mom_drug_search_term_v2\"\n",
    "nows_code_list = ['779.5','P96.1','760.72','P04.14']\n",
    "mom_oud_code_list = \"phenotyping.mprint_mom_oud_code_sheet_5\"\n",
    "opioid_toxicology_code_list = \"phenotyping.mprint_sheet_14_mat_opioid_toxicology\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d9363e5-ee67-4195-bcd2-54f3469c4191",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425a55e0-fc7d-46b3-b9b5-4c84b4a0086c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Change Column name case (lower or upper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6223062-c877-4d72-b62a-83a28afc342a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def change_colname_case(df,case_type):\n",
    "   for col in df.columns:\n",
    "     if (case_type=='upper'):   \n",
    "      df = df.withColumnRenamed(col, col.upper())\n",
    "     elif (case_type=='lower'):   \n",
    "      df = df.withColumnRenamed(col, col.lower()) \n",
    "   return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde68d9b-fc57-454b-a956-9673774fc4be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataframe inspection (counting unique patient and total records; can be used on local and global dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f73cf71a-a95c-4f92-9ac0-89e5af18dc7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_inspection(df_name,sub_group):\n",
    "    \n",
    "    if (sub_group=='all'):\n",
    "           sql = f\"select count(*) AS total, count(distinct mom_person_id) AS unique_mom, count(distinct baby_person_id) AS unique_baby from {df_name};\" \n",
    "    else:\n",
    "           sql = f\"select count(*) AS total ,count(distinct {sub_group}_person_id) AS unique_{sub_group} from {df_name};\" \n",
    "            \n",
    "    if (inspect_df_flag=='df' or inspect_df_flag=='both'): \n",
    "       df= spark.sql(f\"select * from {df_name} limit 10;\")\n",
    "       display(df) \n",
    "        \n",
    "    if (inspect_df_flag=='count' or inspect_df_flag=='both'): \n",
    "       df_info= spark.sql(sql)\n",
    "       display(df_info) \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67a0efde-6cc2-40f3-927b-613d17690114",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## EGA calcuation to get standaraized EGA date informattion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9003407-b273-42d1-81fc-9605e56ea176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cal_ega(SparkDF,ega_col_name):\n",
    "   ega_df = SparkDF.toPandas() ### use pandas function to process (faster)\n",
    "   ega_weeks=[]\n",
    "   ega_days=[]\n",
    "   standardized_ega=[]\n",
    "   total_ega_days=[]\n",
    "\n",
    "    \n",
    "   for index, row in ega_df.iterrows(): ### processed all records\n",
    "       ega=str(row[ega_col_name])\n",
    "       ega_week=0\n",
    "       ega_day=0\n",
    "       found = re.findall('(\\d+\\.?\\d*)', ega) ## 39.6 weeks, 39 weeks, 39 wks 4days. 39 weeks 1/7 -> [39,1,7]\n",
    "       ### normal cases\n",
    "       if (len(found)>=2 ):\n",
    "          ega_week=float(found[0])\n",
    "          ega_day=float(found[1])\n",
    "       elif (len(found)==1 ):\n",
    "         if (\".\" in found[0]): ### 39.6 weeks -> 39 weeks and 6 days\n",
    "          ega_week=int(found[0].split(\".\")[0])\n",
    "          if (found[0].split(\".\")[1].isdigit()):\n",
    "             ega_day=int(found[0].split(\".\")[1])\n",
    "         else: \n",
    "          ega_week=float(found[0])\n",
    "    \n",
    "       ### some exceptions\n",
    "       if (ega_week>99): #303-> 30 weeks and 3 days. 4017-> 40 weeks 1 day. 40 1/7-> 4017. \n",
    "           fstr = repr(ega_week).split('.')[0]\n",
    "           ega_week = float(fstr[:2])\n",
    "           ega_day = float(fstr[2:3])\n",
    "       elif (ega_day>7): # 33-36 weeks -> 33 weeks and 6 days ### Henry: pick mid-point\n",
    "           ega_day=6\n",
    "        \n",
    "       ega_weeks.append(ega_week)\n",
    "       ega_days.append(ega_day)\n",
    "       total_ega_days.append((ega_week*7)+ega_day)\n",
    "       if ega_week > 0 and ega_day > 0:\n",
    "          standardized_ega.append(str(int(ega_week))+\"w \"+str(int(ega_day))+\"d\")\n",
    "       elif ega_week > 0 and ega_day == 0: \n",
    "          standardized_ega.append(str(int(ega_week))+\"w \")\n",
    "       elif ega_week == 0 and ega_day == 0:\n",
    "          standardized_ega.append(None)\n",
    "\n",
    "   ega_df['ega_week']=ega_weeks\n",
    "   ega_df['ega_day']=ega_days\n",
    "   ega_df['total_ega_days']=total_ega_days\n",
    "   ega_df['standardized_ega']=standardized_ega\n",
    "   return spark.createDataFrame(ega_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38c879d-7597-4c35-8d25-633e442a550a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Get mom_baby records with code list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249c96c5-37d2-45c8-b343-d3f60ed9bd9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_records(table_name,code_list, wildcard_sel):\n",
    "    if (table_name == proc_table):\n",
    "        col_name = \"procedure_source_value\"\n",
    "    elif (table_name == cond_table):\n",
    "        col_name = \"condition_source_value\"\n",
    "    elif (table_name == obs_table):\n",
    "        col_name = \"observation_source_value\"\n",
    "\n",
    "    if (wildcard_sel==1):\n",
    "        code_str=' or '.join(map(lambda x: col_name+\" like '\" + x + \"'\", code_list))\n",
    "        sql=f\"select * from {table_name} where {code_str};\"\n",
    "    else:\n",
    "        sql=f\"select * from {table_name} where {col_name} in ({code_list});\"\n",
    "     \n",
    "    table_output = spark.sql(sql)\n",
    "    print(\"Total record:\",table_output.count())\n",
    "    return table_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d96557d4-8f62-4ba6-8dcc-8485f9be401a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mon_baby_records(term,lbc_df_name,subject_id):\n",
    "\n",
    "    if (term==\"condition\"):\n",
    "        date_str=\"condition_start_date\"\n",
    "        datetime_str=\"condition_start_date\"\n",
    "    elif (term==\"procedure\"):\n",
    "        date_str=\"procedure_date\"\n",
    "        datetime_str=\"procedure_datetime\"\n",
    "    elif (term==\"observation\"):\n",
    "        date_str=\"observation_date\"\n",
    "        datetime_str=\"observation_date\"\n",
    "    \n",
    "    sql=f\"\"\"\n",
    "         select FACT_ID_1 as mom_person_id,FACT_ID_2 as baby_person_id,\n",
    "         person_source_value as baby_person_source_value,\n",
    "         birth_datetime,gender_source_value as baby_gender,\n",
    "         race_source_value as baby_race, a.person_id as code_person_id,\n",
    "         {term}_source_value as code,{date_str} as code_date,{datetime_str} as code_datetime from\n",
    "         {lbc_df_name} a inner join global_temp.mom_baby_step1 b\n",
    "         \n",
    "         on a.person_id = b.{subject_id}\n",
    "         and date(birth_datetime) - 30 < {date_str}\n",
    "         and date(birth_datetime) + 30 > {date_str}\n",
    "       \"\"\"\n",
    "    \n",
    "    df = spark.sql(sql)\n",
    "    print(\"term:\",term,\",  record count:\",df.count())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b648f0ae-4c26-4bee-9962-07f44414c845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mon_baby_records_code_list(term,subject_id,code_list):\n",
    "\n",
    "    code_list_str=' , '.join(map(lambda x: \"'\" + x + \"'\", code_list))\n",
    "    if (term==\"condition\"):\n",
    "        date_str=\"condition_start_date\"\n",
    "        datetime_str=\"condition_start_date\"\n",
    "        table_source=cond_table\n",
    "    elif (term==\"procedure\"):\n",
    "        date_str=\"procedure_date\"\n",
    "        datetime_str=\"procedure_datetime\"\n",
    "        table_source=proc_table\n",
    "    elif (term==\"observation\"):\n",
    "        date_str=\"observation_date\"\n",
    "        datetime_str=\"observation_date\"\n",
    "        table_source=obs_table\n",
    "        \n",
    "    sql=f\"\"\"\n",
    "         select FACT_ID_1 as mom_person_id,FACT_ID_2 as baby_person_id,\n",
    "         person_source_value as baby_person_source_value,\n",
    "         birth_datetime,gender_source_value as baby_gender,\n",
    "         race_source_value as baby_race, a.person_id as code_person_id,\n",
    "         {term}_source_value as code,{date_str} as code_date,{datetime_str} as code_datetime from\n",
    "         \n",
    "         (select * from {table_source} where {term}_source_value in ({code_list_str})) a \n",
    "         \n",
    "         inner join global_temp.mom_baby_step1 b\n",
    "         on a.person_id = b.{subject_id}\n",
    "        \"\"\"\n",
    "    df = spark.sql(sql)\n",
    "    print(\"term:\",term,\", record count:\",df.count())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c178c88-dd38-4f7b-be3e-fb264b5e8110",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Union dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3b85fa-c65e-43e6-97da-c7f401b3a168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def union_dataframes(df_list):\n",
    "   # create merged dataframe\n",
    "   df_complete = reduce(DataFrame.unionAll, df_list)\n",
    "   return df_complete.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da057528-643d-40cd-bfee-e65edb0fe266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Output df to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb61e0d-259c-42b2-a89a-5907b49c553b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def file_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae72f18-1ca9-4da3-b4de-61c7dc70a1a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#delete parquet file\n",
    "#shutil.rmtree('/dbfs/FileStore/tables/mom_drug_info_tmp.parquet')\n",
    "\n",
    "def register_parquet_global_view(df):\n",
    "    output_name = df.name\n",
    "    parquet_file=f\"dbfs:/FileStore/tables/{output_name}.parquet\"\n",
    "    \n",
    "    if (output_overwrite or (not file_exists(parquet_file))):\n",
    "       print(\"Start producing/replacing parquet file/view...\") \n",
    "       df.write.mode('overwrite').parquet(parquet_file)\n",
    "       print(\"Done!\")\n",
    "    else:\n",
    "       print(\"Parquet file is ready.\")\n",
    "    \n",
    "    df_parquet = spark.read.parquet(parquet_file)\n",
    "    df_parquet.createOrReplaceGlobalTempView(df.name)\n",
    "    print(\"Parquet view is ready!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "3a7c4330-7663-43a0-95d6-aa0df3a720b3",
     "origId": 1362237165096096,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_project_modules",
   "widgets": {}
  },
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
