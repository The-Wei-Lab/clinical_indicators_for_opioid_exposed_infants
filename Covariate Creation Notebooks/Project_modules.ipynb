{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T19:30:26.568566Z",
     "start_time": "2022-12-26T19:30:26.565306Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f8dda6-84a9-4772-9dcd-ea090463194a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "925e6409-2eba-468f-8d94-0a57b4ce46fa",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "##### Project: Opioid Exposed Infant Covariates\n",
    "##### Investigator: Stephen Patrick, Sarah Loch\n",
    "##### Programmers: Sander Su, Chris Guardo\n",
    "##### Date Created: 01/17/23\n",
    "##### Last Modified: 09/30/25\n",
    "##### Notes:\n",
    "THIS NOTEBOOK REQUIRES ALTERATIONS. Specifically, the section that defines the location of the data in the schema will need to be completed by the user. Reference tables are available in the GitHub Repository\n",
    "\n",
    "###### This file includes MPRINT related libraries:\n",
    "\n",
    "- Change Column name case (lower or upper)\n",
    "- Dataframe inspection (unique patient and total records; local and global)\n",
    "- EGA calcuation to get standaraized EGA days\n",
    "- Read CSV file\n",
    "- Remove widget\n",
    "- Get view information\n",
    "- Aggration Summary (total_count, min, max, mean and median)\n",
    "- Get mom_baby records with code list\n",
    "- Get phenotype_table\n",
    "- Combine two column values into a list\n",
    "- Union dataframes\n",
    "- Cohort definitions (phenotype,baby and mom)\n",
    "- Output df to parquet\n",
    "- Get drug concept id and name\n",
    "- DB Summary\n",
    "- Get search term list\n",
    "- Rename column\n",
    "- Time Travel for different table versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T16:53:45.489473Z",
     "start_time": "2022-12-27T16:53:45.315305Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519e5ba9-ac5d-4df3-b625-857d9902273f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "7b7d432d-da12-4ca1-b42a-71583bf76c71",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "import warnings #ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd # for saving results from SQL queries to pandas DFs\n",
    "import numpy as np # for math operations\n",
    "import re # for regular expression\n",
    "import os\n",
    "import shutil\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import functools\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"auto\")\n",
    "\n",
    "inspect_df_flag = 'count'\n",
    "output_overwrite = True\n",
    "# spark.conf.get(\"spark.databricks.io.cache.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020be46e-27af-42d5-824e-b12804ff6edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###### Table definitions\n",
    "OMOP_database_location = \"***FILL THESE IN***\"\n",
    "refernce_table_database = \"***FILL THESE IN***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87acf358-0790-439f-a733-665df440b51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "proc_table=f\"{OMOP_database_location}.procedure_occurrence\"\n",
    "cond_table=f\"{OMOP_database_location}.condition_occurrence\"\n",
    "obs_table=f\"{OMOP_database_location}.observation\"\n",
    "person_table=f\"{OMOP_database_location}.person\"\n",
    "drug_exp_table=f\"{OMOP_database_location}.drug_exposure\"\n",
    "drug_exp_table_extra=f\"{OMOP_database_location}.x_drug_exposure\"\n",
    "meas_table=f\"{OMOP_database_location}.measurement\"\n",
    "note_table=f\"{OMOP_database_location}.note\"\n",
    "visit_table=f\"{OMOP_database_location}.visit_occurrence\"\n",
    "death_table=f\"{OMOP_database_location}.death\"\n",
    "concept_table=f\"{OMOP_database_location}.concept\"\n",
    "location_table = f\"{OMOP_database_location}.location\"\n",
    "location_history_table = f\"{OMOP_database_location}.location_history\"\n",
    "clin_doc_form_table = f\"{OMOP_database_location}.x_clin_doc_form_fields\"\n",
    "\n",
    "fact_real_table=f\"{OMOP_database_location}.fact_relationship\"\n",
    "insurance_table=f\"{OMOP_database_location}.x_enc_insurance\"\n",
    "smoking_status_table=f\"{OMOP_database_location}.x_smoking_status\"\n",
    "\n",
    "\n",
    "flowsheet_table=f\"{refernce_table_database}.hci_pat_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425a55e0-fc7d-46b3-b9b5-4c84b4a0086c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Change Column name case (lower or upper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6223062-c877-4d72-b62a-83a28afc342a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def change_colname_case(df,case_type):\n",
    "   for col in df.columns:\n",
    "     if (case_type=='upper'):   \n",
    "      df = df.withColumnRenamed(col, col.upper())\n",
    "     elif (case_type=='lower'):   \n",
    "      df = df.withColumnRenamed(col, col.lower()) \n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde68d9b-fc57-454b-a956-9673774fc4be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Dataframe inspection (unique patient and total records; local and global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f73cf71a-a95c-4f92-9ac0-89e5af18dc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_inspection(df_name,sub_group):\n",
    "    \n",
    "    if (sub_group=='all'):\n",
    "           sql = f\"select count(*) AS total, count(distinct mom_person_id) AS unique_mom, count(distinct baby_person_id) AS unique_baby from {df_name};\" \n",
    "    else:\n",
    "           sql = f\"select count(*) AS total ,count(distinct {sub_group}_person_id) AS unique_{sub_group} from {df_name};\" \n",
    "            \n",
    "    if (inspect_df_flag=='df' or inspect_df_flag=='both'): \n",
    "       df= spark.sql(f\"select * from {df_name} limit 10;\")\n",
    "       display(df) \n",
    "        \n",
    "    if (inspect_df_flag=='count' or inspect_df_flag=='both'): \n",
    "       df_info= spark.sql(sql)\n",
    "       display(df_info) \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf89413-0dc0-450e-a520-124d871c0d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### EGA calcuation to get standaraized EGA days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382135c8-2d5f-4d28-851f-9eb70d9e92c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cal_ega(SparkDF,ega_col_name):\n",
    "   ega_df = SparkDF.toPandas() ### use pandas function to process (faster)\n",
    "   ega_weeks=[]\n",
    "   ega_days=[]\n",
    "   standardized_ega=[]\n",
    "   total_ega_days=[]\n",
    "\n",
    "    \n",
    "   for index, row in ega_df.iterrows(): ### processed all records\n",
    "       ega=str(row[ega_col_name])\n",
    "       ega_week=0\n",
    "       ega_day=0\n",
    "       found = re.findall('(\\d+\\.?\\d*)', ega) ## 39.6 weeks, 39 weeks, 39 wks 4days. 39 weeks 1/7 -> [39,1,7]\n",
    "       ### normal cases\n",
    "       if (len(found)>=2 ):\n",
    "          ega_week=float(found[0])\n",
    "          ega_day=float(found[1])\n",
    "       elif (len(found)==1 ):\n",
    "         if (\".\" in found[0]): ### 39.6 weeks -> 39 weeks and 6 days\n",
    "          ega_week=int(found[0].split(\".\")[0])\n",
    "          if (found[0].split(\".\")[1].isdigit()):\n",
    "             ega_day=int(found[0].split(\".\")[1])\n",
    "         else: \n",
    "          ega_week=float(found[0])\n",
    "    \n",
    "       ### some exceptions\n",
    "       if (ega_week>99): #303-> 30 weeks and 3 days. 4017-> 40 weeks 1 day. 40 1/7-> 4017. \n",
    "           fstr = repr(ega_week).split('.')[0]\n",
    "           ega_week = float(fstr[:2])\n",
    "           ega_day = float(fstr[2:3])\n",
    "       elif (ega_day>7): # 33-36 weeks -> 33 weeks and 6 days ### Henry: pick mid-point\n",
    "           ega_day=6\n",
    "        \n",
    "       ega_weeks.append(ega_week)\n",
    "       ega_days.append(ega_day)\n",
    "       total_ega_days.append((ega_week*7)+ega_day)\n",
    "       if ega_week > 0 and ega_day > 0:\n",
    "          standardized_ega.append(str(int(ega_week))+\"w \"+str(int(ega_day))+\"d\")\n",
    "       elif ega_week > 0 and ega_day == 0: \n",
    "          standardized_ega.append(str(int(ega_week))+\"w \")\n",
    "       elif ega_week == 0 and ega_day == 0:\n",
    "          standardized_ega.append(None)\n",
    "\n",
    "   ega_df['ega_week']=ega_weeks\n",
    "   ega_df['ega_day']=ega_days\n",
    "   ega_df['total_ega_days']=total_ega_days\n",
    "   ega_df['standardized_ega']=standardized_ega\n",
    "   return spark.createDataFrame(ega_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0eaef75-b85e-4a16-97e3-731ae2f31ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Read CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46103f47-fb4f-4045-9456-109c5fb4c1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_file(DataFilePath):\n",
    "   \n",
    "   file_df= (spark.read\n",
    "     .format(\"csv\")\n",
    "     .option(\"header\", \"true\")\n",
    "     .option(\"inferSchema\", \"true\")        \n",
    "     .load(DataFilePath)\n",
    "   )\n",
    "   return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bcdef58-db89-456c-b063-d36f60251ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Read CPT/ICD LBC codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T16:53:53.354573Z",
     "start_time": "2022-12-27T16:53:53.160931Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a10b4ab-1668-4339-a4b7-057d496b6112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "2697624e-0856-4a87-9714-303fc6edadd1",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "# def lbc_codes():\n",
    "#    lbc=read_csv_file(\"dbfs:/FileStore/LBC_list_10_20_22.csv\")\n",
    "#    lbc_CPT=lbc[lbc.VOCABULARY_ID=='CPT4'] ### CPT\n",
    "#    lbc_ICD= lbc[lbc.VOCABULARY_ID!='CPT4'] ### ICD (ALL)\n",
    "\n",
    "#    lbc_CPT_list=lbc_CPT.select('CONCEPT_CODE').distinct()\n",
    "#    lbc_ICD_list=lbc_ICD.select('CONCEPT_CODE').distinct()\n",
    "#    print(\"Total CPT code count:\",lbc_CPT_list.count())\n",
    "#    print(\"Total ICD code count:\",lbc_ICD_list.count())\n",
    "\n",
    "#    lbc_CPT_str=lbc_CPT.agg(F.concat_ws(\",\",F.collect_list(F.concat(F.lit('\"'),F.col('CONCEPT_CODE'),F.lit('\"'))))).first()[0]    \n",
    "#    lbc_ICD_str=lbc_ICD.agg(F.concat_ws(\",\",F.collect_list(F.concat(F.lit('\"'),F.col('CONCEPT_CODE'),F.lit('\"'))))).first()[0]\n",
    "#    return lbc_CPT_str,lbc_ICD_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d22a1f-5f91-49e6-b25f-c7feca432ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Get view information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10585333-7b40-43b3-bbc8-497a4c43ede0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_global_temp_output():\n",
    "   sql = \"SHOW VIEWS FROM global_temp;\";\n",
    "   view_names= spark.sql(sql).select('viewName').rdd.flatMap(lambda x: x).collect()\n",
    "   view_names.append('N')\n",
    "   return view_names\n",
    "\n",
    "def temp_output(df_name):\n",
    "    sql = f\"SELECT * FROM global_temp.{df_name};\"\n",
    "    tmp_output= spark.sql(sql)\n",
    "    return tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1bf287a-f688-4dae-8f64-17bc6c540442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Aggration Summary (total_count, min, max, mean and median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb08a30-ddef-41ee-ac96-9847e18101e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_agg_summary(sum_df,agg_cols,target_col):\n",
    "   summary=sum_df.groupBy(agg_cols) \\\n",
    "    .agg(count(target_col).alias(\"TOTAL_RECORD\"),\n",
    "         min(target_col).alias(\"MIN_VAL\"),\n",
    "         round(avg(target_col),2).alias(\"AVG_VAL\"),\n",
    "         round(func.percentile_approx(target_col, 0.5),2).alias(\"MEDIAN_VAL\"),\n",
    "         max(target_col).alias(\"MAX_VAL\")\n",
    "     )\\\n",
    "    .sort(agg_cols,ascending=True) \\\n",
    "    .display()\n",
    "    \n",
    "def df_agg_summary_simple(sum_df,target_col):\n",
    "    summary=sum_df.select(max(target_col).alias(\"MAX_VAL\"), \n",
    "          min(target_col).alias(\"MIN_VAL\"),\n",
    "          round(mean(target_col),2).alias(\"MEAN_VAL\"),                 \n",
    "          round(func.percentile_approx(target_col, 0.5),2).alias(\"MEDIAN_VAL\")                  \n",
    "    ).display()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38c879d-7597-4c35-8d25-633e442a550a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Get mom_baby records with code list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249c96c5-37d2-45c8-b343-d3f60ed9bd9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_records(table_name,col_name,code_list, wildcard_sel):\n",
    "    if (wildcard_sel==1):\n",
    "        code_str=' or '.join(map(lambda x: col_name+\" like '\" + x + \"'\", code_list))\n",
    "        sql=f\"select * from {table_name} where {code_str};\"\n",
    "    else:\n",
    "        sql=f\"select * from {table_name} where {col_name} in ({code_list});\"\n",
    "     \n",
    "    table_output = spark.sql(sql)\n",
    "    print(\"Total record:\",table_output.count())\n",
    "    return table_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d96557d4-8f62-4ba6-8dcc-8485f9be401a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mon_baby_records(term,lbc_df_name,subject_id):\n",
    "    if (term==\"condition\"):\n",
    "        date_str=\"condition_start_date\"\n",
    "        datetime_str=\"condition_start_date\"\n",
    "    elif (term==\"procedure\"):\n",
    "        date_str=\"procedure_date\"\n",
    "        datetime_str=\"procedure_datetime\"\n",
    "    elif (term==\"observation\"):\n",
    "        date_str=\"observation_date\"\n",
    "        datetime_str=\"observation_date\"\n",
    "    \n",
    "    sql=f\"\"\"\n",
    "         select FACT_ID_1 as mom_person_id,FACT_ID_2 as baby_person_id,\n",
    "         person_source_value as baby_person_source_value,\n",
    "         birth_datetime,gender_source_value as baby_gender,\n",
    "         race_source_value as baby_race, a.person_id as code_person_id,\n",
    "         {term}_source_value as code,{date_str} as code_date,{datetime_str} as code_datetime from\n",
    "         {lbc_df_name} a inner join global_temp.mom_baby_step1 b\n",
    "         \n",
    "         on a.person_id = b.{subject_id}\n",
    "         and date(birth_datetime) - 30 < {date_str}\n",
    "         and date(birth_datetime) + 30 > {date_str}\n",
    "       \"\"\"\n",
    "    \n",
    "    df = spark.sql(sql)\n",
    "    print(\"term:\",term,\", record count:\",df.count())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b648f0ae-4c26-4bee-9962-07f44414c845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mon_baby_records_code_list(term,subject_id,code_list):\n",
    "\n",
    "    code_list_str=' , '.join(map(lambda x: \"'\" + x + \"'\", code_list))\n",
    "    if (term==\"condition\"):\n",
    "        date_str=\"condition_start_date\"\n",
    "        datetime_str=\"condition_start_date\"\n",
    "        table_source=cond_table\n",
    "    elif (term==\"procedure\"):\n",
    "        date_str=\"procedure_date\"\n",
    "        datetime_str=\"procedure_datetime\"\n",
    "        table_source=proc_table\n",
    "    elif (term==\"observation\"):\n",
    "        date_str=\"observation_date\"\n",
    "        datetime_str=\"observation_date\"\n",
    "        table_source=obs_table\n",
    "        \n",
    "    sql=f\"\"\"\n",
    "         select FACT_ID_1 as mom_person_id,FACT_ID_2 as baby_person_id,\n",
    "         person_source_value as baby_person_source_value,\n",
    "         birth_datetime,gender_source_value as baby_gender,\n",
    "         race_source_value as baby_race, a.person_id as code_person_id,\n",
    "         {term}_source_value as code,{date_str} as code_date,{datetime_str} as code_datetime from\n",
    "         \n",
    "         (select * from {table_source} where {term}_source_value in ({code_list_str})) a \n",
    "         \n",
    "         inner join global_temp.mom_baby_step1 b\n",
    "         on a.person_id = b.{subject_id}\n",
    "        \"\"\"\n",
    "    df = spark.sql(sql)\n",
    "    print(\"term:\",term,\", record count:\",df.count())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7e7df85-950f-4a4a-b93e-27135e2cfa0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Get phenotype_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a414c71c-fb8c-4881-a8a0-042d3ca61164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table(version):\n",
    "    if (version=='V1_05232022'):\n",
    "      table_name = \"workspace_rdmprintp2.phenotyping.mom_baby_2010_mombabypair_all_version_1_05232022\";\n",
    "    elif (version=='V2_11042022'):\n",
    "      table_name = \"workspace_rdmprintp2.phenotyping.mom_baby_2010_mombabypair_all_version_1_11042022_0523202\";\n",
    "    elif (version=='Current_version'):\n",
    "      table_name = \"global_temp.mom_baby_2010_mombabypair_all_current\"; \n",
    "    \n",
    "    return table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ad8d89-7e37-4e79-8eb3-1e2881d09844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Combine two column values into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b588cae-0485-4024-8f35-61cb6b89af71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def combine_column_value(sheet_name,col_name1,col_name2):\n",
    "   \n",
    "   sql=f\"select * from {sheet_name};\"\n",
    "   df = spark.sql(sql)\n",
    "   \n",
    "   name_list=df.select([lower(col_name1),lower(col_name2)]).rdd.flatMap(lambda x: x).distinct().collect()\n",
    "   while(None in name_list):\n",
    "    name_list.remove(None)\n",
    "   print(\"total record count in list:\",len(name_list))\n",
    "   \n",
    "   return name_list,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c178c88-dd38-4f7b-be3e-fb264b5e8110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Union dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3b85fa-c65e-43e6-97da-c7f401b3a168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def union_dataframes(df_list):\n",
    "   # create merged dataframe\n",
    "   df_complete = functools.reduce(DataFrame.unionAll, df_list)\n",
    "   return df_complete.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741f7b1f-d8eb-4a3f-b42f-a144b126fc21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Cohort definitions (phenotype,baby and mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b60e662-3813-4c58-99f3-eb7fba3e55ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_whole_phenotype_cohort(phenotype_table):\n",
    "  \n",
    "   sql=f\"\"\"\n",
    "        select * from {phenotype_table};\n",
    "      \"\"\"\n",
    "   phenotype_cohort= spark.sql(sql)\n",
    "   \n",
    "   return phenotype_cohort\n",
    "\n",
    "def get_phenotype_cohort(phenotype_table):\n",
    "  \n",
    "   sql=f\"\"\"\n",
    "        select * from {phenotype_table}\n",
    "        where gestational_age_w33_or_uncertain = 1 and live_birth_code=1 and critical_illness_4cpt = 0 and \n",
    "        respiratory_procedure_code = 0 and fetal_anomalies_code =0;\n",
    "      \"\"\"\n",
    "   phenotype_cohort= spark.sql(sql)\n",
    "   \n",
    "   return phenotype_cohort\n",
    "\n",
    "def get_baby_exposed_opioid(phenotype_table):\n",
    "    sql=f\"\"\"\n",
    "       select * from {phenotype_table} where gestational_age_w33_or_uncertain = 1 and live_birth_code=1 and \n",
    "       critical_illness_4cpt = 0 and respiratory_procedure_code = 0 and fetal_anomalies_code =0 \n",
    "       and (nows_baby_code =1 or infant_tox_lab =1);\n",
    "    \"\"\"\n",
    "    exposed_opioid_infant = spark.sql(sql)\n",
    "    return exposed_opioid_infant\n",
    "\n",
    "def get_baby_bh_cohort(phenotype_table,exposed_opioid):\n",
    "    \n",
    "   if (exposed_opioid=='opioid_exposed'):\n",
    "      infant_cohort=get_baby_exposed_opioid(phenotype_table)\n",
    "   else:     \n",
    "      infant_cohort=get_phenotype_cohort(phenotype_table)\n",
    "        \n",
    "   infant_cohort.createOrReplaceTempView(\"infant_cohort\")\n",
    "\n",
    "   sql=\"\"\"\n",
    "         select * from global_temp.mom_baby_step1_baby1stvisit where baby_person_id in (select baby_person_id from infant_cohort);\n",
    "       \"\"\"\n",
    "   baby_bh_cohort= spark.sql(sql)\n",
    "   return baby_bh_cohort\n",
    "\n",
    "def get_mom_cohort(phenotype_table):\n",
    "    sql=f\"\"\"\n",
    "          select *\n",
    "          from {phenotype_table}\n",
    "          where gestational_age_w33_or_uncertain = 1 and live_birth_code=1 and critical_illness_4cpt = 0 \n",
    "          and respiratory_procedure_code = 0 and fetal_anomalies_code =0 \n",
    "          and (mom_oud_inpatient = 1 or mom_oud_outpatient=1 or mom_drug = 1 or mom_drug_in_note = 1);\n",
    "        \"\"\"\n",
    "\n",
    "    mom_exposed_opioid = spark.sql(sql)\n",
    "    return mom_exposed_opioid\n",
    "\n",
    "def get_baby_brestfeed_cohort(phenotype_table):\n",
    "    \n",
    "    sql=f\"\"\"\n",
    "          select MOM_PERSON_SOURCE_VALUE,AGE_AT_DELIVERY,MOM_BIRTH_DATETIME,MOM_GENDER,MOM_RACE,BABY_PERSON_ID,\n",
    "          BABY_PERSON_SOURCE_VALUE,BABY_BIRTH_DATETIME,BABY_GENDER,BABY_RACE,LIVE_BIRTH_CODE,PREGNANCY_CODE,\n",
    "          GESTATIONAL_AGE_W33_OR_UNCERTAIN,CRITICAL_ILLNESS_4CPT,RESPIRATORY_PROCEDURE_CODE,\n",
    "          FETAL_ANOMALIES_CODE,GESTATIONAL_AGE_UNCERTAIN,NOWS_BABY_CODE,INFANT_TOX_LAB,MOM_OUD,\n",
    "          MOM_OUD_INPATIENT,MOM_OUD_OUTPATIENT,MOM_DRUG,MOM_DRUG_IN_NOTE,MOM_OPIOID_TOX from {phenotype_table} where\n",
    "       \n",
    "          gestational_age_w33_or_uncertain = 1 and live_birth_code=1 and critical_illness_4cpt = 0 and \n",
    "          respiratory_procedure_code = 0 and fetal_anomalies_code =0 and (nows_baby_code = 1 or \n",
    "          infant_tox_lab =1 or mom_oud = 1 or mom_oud_inpatient = 1 or mom_oud_outpatient = 1 or mom_drug=1 or mom_drug_in_note =1)\n",
    "         \"\"\"\n",
    "\n",
    "    baby_brestfeed_cohort = spark.sql(sql)\n",
    "    return baby_brestfeed_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da057528-643d-40cd-bfee-e65edb0fe266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Output df to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb61e0d-259c-42b2-a89a-5907b49c553b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def file_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae72f18-1ca9-4da3-b4de-61c7dc70a1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#delete parquet file\n",
    "# shutil.rmtree('/dbfs/FileStore/tables/mom_drug_info_tmp.parquet')\n",
    "\n",
    "def register_parquet_global_view(df):\n",
    "    output_name=df.name\n",
    "    parquet_file=\"dbfs:/FileStore/tables/\"+output_name+\".parquet\"\n",
    "    \n",
    "    if (output_overwrite or (not file_exists(parquet_file))):\n",
    "       print(\"Start producing/replacing parquet file/view...\") \n",
    "       df.write.mode('overwrite').parquet(parquet_file)\n",
    "       print(\"Done!\")\n",
    "    else:\n",
    "       print(\"Parquet file is ready.\")\n",
    "    \n",
    "    df_parquet = spark.read.parquet(parquet_file)\n",
    "    df_parquet.createOrReplaceGlobalTempView(df.name)\n",
    "    print(\"Parquet view is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040eec23-c2a7-4272-bfa5-0d3bdff015b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Get drug concept id and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a78c4d-777b-4750-b65e-eea4ba3af128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_drug_concept_id_name(sheet_name,col_name):\n",
    "    sql=f\"select lower({col_name}) as {col_name} from {sheet_name} order by {col_name} asc;\"\n",
    "    drug_search_term = spark.sql(sql)\n",
    "    drug_str=drug_search_term.agg(F.concat_ws(\" or \",F.collect_list(F.concat(F.lit('lower(concept_name) LIKE \"%'),F.col(col_name),F.lit('%\"'))))).first()[0] \n",
    "    \n",
    "    sql=f\"select concept_id,concept_name from {concept_table} where {drug_str};\"\n",
    "    \n",
    "    concept_id_name_tmp=spark.sql(sql)\n",
    "    return concept_id_name_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6985221-6c38-4c26-b45a-df1b32cfc077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### DB Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "832c59e4-82f7-4179-99f3-99a9381dfd32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def db_summary():\n",
    "  final_list = []\n",
    "  dbList = spark.sql(\"show databases\").select(\"databaseName\").rdd.flatMap(lambda x: x).collect()\n",
    "  for databaseName in dbList:\n",
    "    spark.sql(\"use {}\".format(databaseName))\n",
    "    tableList = spark.sql(\"show tables from {}\".format(databaseName)).select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    for tableName in tableList:\n",
    "      tableCount = spark.sql(\"select count(*) as tableCount from {}\".format(tableName)).collect()[0][0]\n",
    "      final_list.append(list([databaseName,tableName,tableCount]))\n",
    "  column_names = list(['DatabaseName','TableName','TableCount'])\n",
    "  df = spark.createDataFrame(final_list,column_names)\n",
    "  df=df.sort(df.DatabaseName.asc(),df.TableCount.desc())\n",
    "  \n",
    "  display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef3526a-a1f6-48e3-b050-9b6db8212942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#db_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f4b258-f133-482a-8433-c85268527200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Get searah term list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dfda1d4-6967-4d7b-adf1-283aab20e500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_search_term(sheet_name):\n",
    "    \n",
    "    sql=f\"\"\"\n",
    "       select lab,search_term from (\n",
    "          select lab,lower(short_name) as search_term from {sheet_name}\n",
    "          union\n",
    "          select lab,lower(long_name) as search_term from {sheet_name}\n",
    "       ) where search_term is not null\n",
    "    \"\"\"\n",
    "    search_term_list=spark.sql(sql)\n",
    "    return search_term_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33dd0601-2867-46eb-b475-6286fb688666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b89de20-2e9d-4705-88e8-4ba1f1ad3635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def rename_column(source_df,name_dict):\n",
    "    for key, value in name_dict.items():\n",
    "      source_df= source_df.withColumnRenamed(key,value)\n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78dd4bb-89c9-4d33-8422-0d8294777514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Time Travel for different table versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee1e1a1-3429-4bfc-bb6a-0c148f921de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def version_diff(tracking_table):\n",
    "    sql=f\"\"\"\n",
    "        select version from (\n",
    "           DESCRIBE HISTORY {tracking_table}\n",
    "        ) limit 2;\n",
    "        \"\"\"\n",
    "    version_info=spark.sql(sql)\n",
    "    if (version_info.count()==1):\n",
    "        print(\"This is the only version\")\n",
    "    else:\n",
    "        row_list = version_info.collect()\n",
    "        new_version=row_list[0][0]\n",
    "        old_version=row_list[1][0]\n",
    "        \n",
    "        sql=f\"\"\"\n",
    "              select * from {tracking_table}@v{new_version}\n",
    "              except all\n",
    "              select * from\n",
    "              {tracking_table}@v{old_version}\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        diff_df=spark.sql(sql)\n",
    "        print(\"Total diff record count:\",diff_df.count())\n",
    "        return diff_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "3a7c4330-7663-43a0-95d6-aa0df3a720b3",
     "origId": 4464768789707858,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Project_modules",
   "widgets": {
    "param_1": {
     "currentValue": "",
     "nuid": "cc00f518-671a-48ee-ae2f-a20b221ac0aa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "param_1",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "param_1",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}